{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as plt_image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score\n",
    "\n",
    "from pathlib import Path\n",
    "Path.ls = lambda p: list(p.iterdir())\n",
    "\n",
    "import torch\n",
    "import torch.nn as NN\n",
    "from torch.optim import Optimizer\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "NO_FINDING = \"No Finding\"\n",
    "PATHOLOGIC = \"pathologic\"\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "SUBSET_MEAN = 0.50589985\n",
    "SUBSET_STD = 0.23221017\n",
    "\n",
    "LABEL_DF = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def seed_everything(seed=92):\n",
    "    try: random.seed(seed)\n",
    "    except: pass\n",
    "    try: np.random.seed(seed)\n",
    "    except: pass\n",
    "    try: torch.manual_seed(seed)\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ignore_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_working_dir():\n",
    "    return Path(f\"{os.environ.get('HOME')}/work/crx8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_data_path():\n",
    "    return Path(f\"{os.environ.get('HOME')}/.datasets/CRX8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_model_path():\n",
    "    return get_working_dir()/\"pt_models\"\n",
    "\n",
    "def save_model(model, model_name):\n",
    "    path = str(get_model_path()/f\"{model_name}.pt\")\n",
    "    torch.save(model, path)\n",
    "\n",
    "def load_model(model_name):\n",
    "    path = str(get_model_path()/f\"{model_name}.pt\")\n",
    "    return torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def FERTIG():\n",
    "    print(\"FERTIG! :D\")\n",
    "    with open(get_working_dir()/\"fertig.txt\", \"w\") as f:\n",
    "        f.write(\"FERTIG! :D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_image_path(): return get_data_path()/\"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_train_val_list(): \n",
    "    with open(get_data_path()/\"train_val_list.txt\") as f:\n",
    "        train_val_list = f.readlines()\n",
    "    return [l.replace(\"\\n\", \"\") for l in train_val_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_labels(reduced=True):\n",
    "    data = pd.read_csv(get_data_path()/\"Data_Entry_2017_v2020.csv\")\n",
    "    tmp = data[\"Finding Labels\"].values\n",
    "    labels = set()\n",
    "    for el in tmp:\n",
    "        for l in el.split(\"|\"): labels.add(l)\n",
    "    if reduced: labels = [l for l in list(labels) if l != \"No Finding\"]\n",
    "    return list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dataframes(reduced=True, small=False, small_fraction=0.1, include_labels=get_labels()):\n",
    "    image_dir = get_image_path()\n",
    "    train_df, test_df = get_label_dfs(reduced=reduced)\n",
    "    train_df, valid_df = get_train_valid(train_df)\n",
    "    \n",
    "    if small:\n",
    "        train_idx = int(train_df.shape[0] * small_fraction)\n",
    "        val_idx   = int(valid_df.shape[0] * small_fraction)\n",
    "        test_idx  = int(test_df.shape[0]  * small_fraction)\n",
    "        \n",
    "        train_df = train_df.iloc[:train_idx,:]\n",
    "        valid_df = valid_df.iloc[:val_idx,:]\n",
    "        test_df  = test_df.iloc[:test_idx,:]\n",
    "    \n",
    "    if len(include_labels) < len(get_labels()):\n",
    "    \n",
    "        tmp_train_df = train_df[train_df[include_labels].sum(axis=1, numeric_only=True) > 0]\n",
    "        tmp_valid_df = valid_df[valid_df[include_labels].sum(axis=1, numeric_only=True) > 0]\n",
    "        tmp_test_df  = test_df[test_df[include_labels].sum(axis=1, numeric_only=True) > 0]\n",
    "\n",
    "        negative_df = pd.concat([train_df, valid_df], axis=0)\n",
    "        #display(negative_df)\n",
    "        \n",
    "        negative_df = negative_df[negative_df[get_labels()].sum(axis=1) == 0]\n",
    "        test_neg_df = test_df[test_df[include_labels].sum(axis=1, numeric_only=True) == 0]\n",
    "        \n",
    "        pos_rows = tmp_train_df.shape[0] + tmp_valid_df.shape[0]\n",
    "        if negative_df.shape[0] < pos_rows:\n",
    "            train_idx = math.floor(negative_df.shape[0] * tmp_train_df.shape[0] / pos_rows)\n",
    "            print(train_idx)\n",
    "            valid_idx = math.floor(train_idx + negative_df.shape[0] * tmp_valid_df.shape[0] / pos_rows)\n",
    "        else:\n",
    "            train_idx = tmp_train_df.shape[0]\n",
    "            valid_idx = train_idx + tmp_valid_df.shape[0]\n",
    "        \n",
    "        \n",
    "        train_df = pd.concat([tmp_train_df, negative_df.iloc[:train_idx, :]], axis=0)\n",
    "        valid_df = pd.concat([tmp_valid_df, negative_df.iloc[train_idx:valid_idx, :]], axis=0)\n",
    "        \n",
    "        if tmp_test_df.shape[0] >= test_neg_df.shape[0]:\n",
    "            test_df = pd.concat([tmp_test_df, test_neg_df], axis=1)\n",
    "        else:\n",
    "            test_df = pd.concat([tmp_test_df, test_neg_df.iloc[:tmp_test_df.shape[0], :]], axis=0)\n",
    "        \n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_label_dfs(reduced=True):\n",
    "    data = pd.read_csv(get_data_path()/\"Data_Entry_2017_v2020.csv\")\n",
    "    additional_data = data.drop(columns=[\"Image Index\", \"Finding Labels\"])\n",
    "    values = data.values[:,:2]\n",
    "    labels = get_labels(reduced=reduced)\n",
    "    cols = [\"Image Index\", *labels]\n",
    "    col2idx = {c:i for i, c in enumerate(labels)}\n",
    "    arr = np.zeros((values.shape[0], len(labels)))\n",
    "\n",
    "    for row in range(arr.shape[0]):\n",
    "        image_labels = values[row, 1].split(\"|\")\n",
    "        for col, col_name in enumerate(cols):\n",
    "            for lbl in image_labels:\n",
    "                if reduced:\n",
    "                    if lbl == NO_FINDING: continue\n",
    "                arr[row, col2idx[lbl]] = 1\n",
    "\n",
    "    new_data = pd.DataFrame({\"Image Index\": values[:,0]})\n",
    "    new_data = pd.concat([new_data, pd.DataFrame(arr, columns=labels), additional_data], axis=1)\n",
    "    \n",
    "    train_label_df = new_data[new_data[\"Image Index\"].isin(get_train_val_list())]\n",
    "    test_label_df = new_data[new_data[\"Image Index\"].isin(get_test_list()) ]\n",
    "    \n",
    "    return train_label_df, test_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def check_for_leakage(df1, df2):\n",
    "    patient_col = \"Patient ID\"\n",
    "    df1_patients_unique = set(df1[patient_col].values)\n",
    "    df2_patients_unique = set(df2[patient_col].values)\n",
    "    patients_in_both_groups = df1_patients_unique.intersection(df2_patients_unique)\n",
    "    leakage = len(patients_in_both_groups) > 0\n",
    "    return leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_rows(indices, q_d, data_d):\n",
    "    for r_idx in indices:\n",
    "        for c_idx, k in enumerate(data_d.keys()): \n",
    "            data_d[k].append(q_d[r_idx, c_idx])\n",
    "    return data_d\n",
    "\n",
    "def get_train_valid(df, val_size=0.2):\n",
    "    \n",
    "    try:\n",
    "        train_df = pd.read_csv(\n",
    "            get_working_dir()/\"no_overlap_train_df.csv\", index_col=\"Unnamed: 0\")\n",
    "        valid_df = pd.read_csv(\n",
    "            get_working_dir()/\"no_overlap_valid_df.csv\", index_col=\"Unnamed: 0\")\n",
    "        return train_df, valid_df\n",
    "    except:\n",
    "        print(\"No precomputed dataframes found...\\nComputing them now!\")\n",
    "    \n",
    "    patients = Counter(df[\"Patient ID\"].values).most_common()\n",
    "\n",
    "    train_size = round(df.shape[0] * (1 - val_size))\n",
    "    valid_size = df.shape[0] - train_size\n",
    "    patient_id_idx = [i+1 for i, k in enumerate(df.columns) if k == \"Patient ID\"][0]\n",
    "\n",
    "    train_df = {k: [] for k in [\"tmp_index\", *df.columns]}\n",
    "    valid_df = {k: [] for k in [\"tmp_index\", *df.columns]}\n",
    "\n",
    "    df_values = df.values\n",
    "    tmp = np.zeros((df.shape[0], df.shape[1]+1)).astype(np.object)\n",
    "    tmp[:, 1:] = df_values\n",
    "    tmp[:, 0] = df.index.values\n",
    "    df_values = tmp\n",
    "\n",
    "    for pid, _ in tqdm(patients):\n",
    "        train_fill = len(train_df[\"tmp_index\"]) / train_size\n",
    "        valid_fill = len(valid_df[\"tmp_index\"]) / valid_size\n",
    "        if train_fill <= valid_fill:\n",
    "            indices = np.where(df_values[:,patient_id_idx] == pid)[0]\n",
    "            train_df = add_rows(indices, df_values, train_df)\n",
    "        else:\n",
    "            indices = np.where(df_values[:,patient_id_idx] == pid)[0]\n",
    "            valid_df = add_rows(indices, df_values, valid_df)\n",
    "    \n",
    "    train_df = pd.DataFrame(train_df)\n",
    "    tmp_idx = train_df[\"tmp_index\"].values\n",
    "    train_df = train_df.drop(columns=\"tmp_index\")\n",
    "    train_df.index= tmp_idx\n",
    "    \n",
    "    valid_df = pd.DataFrame(valid_df)\n",
    "    tmp_idx = valid_df[\"tmp_index\"].values\n",
    "    valid_df = valid_df.drop(columns=\"tmp_index\")\n",
    "    valid_df.index= tmp_idx\n",
    "    \n",
    "    train_df.to_csv(get_working_dir()/\"no_overlap_train_df.csv\", index=True)\n",
    "    valid_df.to_csv(get_working_dir()/\"no_overlap_valid_df.csv\", index=True)\n",
    "    print(\"Dataframes saved!\")\n",
    "    \n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def old_get_train_valid(data, val_size=0.2, seed=92):\n",
    "    # Currently with patient overlap!\n",
    "    warnings.warn(\"Train-Val-Split with patient overlap!\")\n",
    "    warnings.warn(\"DEPRECATED\")\n",
    "    labels = get_labels()\n",
    "    X = data[[c for c in data.columns if c not in labels]]\n",
    "    y = data[labels]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=val_size, random_state=seed)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T19:19:10.014412Z",
     "start_time": "2021-03-11T19:19:10.011001Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_label_df():\n",
    "    global LABEL_DF\n",
    "    if LABEL_DF is None: \n",
    "        LABEL_DF = pd.read_csv(get_data_path()/\"Data_Entry_2017_v2020.csv\")\n",
    "        to_drop = [c for c in LABEL_DF.columns if c not in [\"Image Index\", \"Finding Labels\"]]\n",
    "        LABEL_DF = LABEL_DF.drop(columns=to_drop)\n",
    "    return LABEL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def image2label(fn):\n",
    "    label_df = load_label_df()\n",
    "    return label_df[label_df[\"Image Index\"] == fn][\"Finding Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def translate2label(arr, labels=None):\n",
    "    return \"not implemented\"\n",
    "    try: arr = arr.values[0]\n",
    "    except: pass\n",
    "    if labels is None: labels = get_labels()\n",
    "    idx2lbl = {i: l for i, l in enumerate(labels)}\n",
    "    pos_lbls = [idx2lbl[idx] for idx, v in enumerate(arr) if v == 1]\n",
    "    print(pos_lbls)\n",
    "    if len(pos_lbls) == 0: return \"No Finding\"\n",
    "    return \"|\".join(pos_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_image(x, label=\"(no value passed)\"):\n",
    "    assert len(x.shape) == 3\n",
    "    \n",
    "    x = np.array(x)\n",
    "    x = np.einsum(\"cwh -> whc\", x)\n",
    "    \n",
    "    #c, w, h = x.shape\n",
    "    x = x * IMAGENET_STD + IMAGENET_MEAN\n",
    "    plt.title(label)\n",
    "    plt.imshow(x, cmap=\"gray\")\n",
    "    #try: plt.imshow(x.view(w, h, c), cmap=\"bone\");\n",
    "    #except: plt.imshow(x.reshape(w, h, c), cmap=\"bone\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "label_str = lambda v, l: l if v > 0.5 else NO_FINDING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def translate_one_label(label, values):\n",
    "    translate_label = lambda v: label if v > 0.5 else NO_FINDING\n",
    "    return list(map(translate_label, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_one_label_batch(X, y, label):\n",
    "\n",
    "    assert X.shape[0] % 2 == 0\n",
    "    X = X * SUBSET_STD + SUBSET_MEAN\n",
    "    x_dim = int(X.shape[0] / 2)\n",
    "    y_dim = int(X.shape[0] / x_dim)\n",
    "    \n",
    "    axes = []\n",
    "    figure = plt.figure(figsize=(2*x_dim, 20*y_dim))\n",
    "    i = 0\n",
    "    for y_idx in range(y_dim):\n",
    "        for x_idx in range(x_dim):\n",
    "            axes.append(figure.add_subplot(x_dim, y_dim, i+1))\n",
    "            axes[-1].set_title(translate_one_label(label, y[i]))\n",
    "            plt.imshow(X[i].reshape(X[i].shape[1], X[i].shape[2], X[i].shape[0]))\n",
    "            i += 1\n",
    "    figure.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_batch(X, y, labels=get_labels()):\n",
    "    assert X.shape[0] % 2 == 0\n",
    "    X = X * SUBSET_STD + SUBSET_MEAN\n",
    "    x_dim = int(X.shape[0] / 2)\n",
    "    y_dim = int(X.shape[0] / x_dim)\n",
    "    \n",
    "    axes = []\n",
    "    figure = plt.figure(figsize=(2*x_dim, 20*y_dim))\n",
    "    i = 0\n",
    "    for y_idx in range(y_dim):\n",
    "        for x_idx in range(x_dim):\n",
    "            axes.append(figure.add_subplot(x_dim, y_dim, i+1))\n",
    "            axes[-1].set_title(translate2label(y[i]))\n",
    "            plt.imshow(X[i].reshape(X[i].shape[1], X[i].shape[2], X[i].shape[0]))\n",
    "            i += 1\n",
    "    figure.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_test_list(): \n",
    "    with open(get_data_path()/\"test_list.txt\") as f:\n",
    "        train_val_list = f.readlines()\n",
    "    return [l.replace(\"\\n\", \"\") for l in train_val_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_transforms(image_size=(224, 224)):\n",
    "    train_tfs = transforms.Compose([\n",
    "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(1)\n",
    "    ])\n",
    "    test_tfs = transforms.Compose([\n",
    "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "        transforms.Resize(image_size)\n",
    "    ])\n",
    "    return train_tfs, test_tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_class_freqs(labels):\n",
    "    N = labels.shape[0]\n",
    "    \n",
    "    positive_frequencies = np.sum(labels, axis=0) / N\n",
    "    negative_frequencies = 1 - positive_frequencies\n",
    "\n",
    "    return positive_frequencies, negative_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_positive_class_weigths(labels):\n",
    "    n_positives = (labels == 1).sum(axis=0)\n",
    "    n_negatives = (labels == 0).sum(axis=0)\n",
    "    return torch.Tensor(n_negatives / n_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def auroc(y_hat, y, model_name=\"model_name\", with_chexnet=True, with_previous=True):\n",
    "    aurocs = {}\n",
    "    for l_idx, l in enumerate(get_labels()):\n",
    "        try:\n",
    "            v = roc_auc_score(y[:, l_idx], y_hat[:, l_idx])\n",
    "        except ValueError:\n",
    "            warnings.warn(f\"{l} only has one class. Returning 0!\")\n",
    "            v = 0.\n",
    "        aurocs[l] = v\n",
    "    df = pd.DataFrame(aurocs.values(), index=aurocs.keys(), columns=[model_name])\n",
    "    if with_previous:\n",
    "        prev = load_results()\n",
    "        for i, c in enumerate(prev.columns):\n",
    "            if c not in df.columns:\n",
    "                df = pd.concat([df, prev.iloc[:, i]], axis=1)\n",
    "    if with_chexnet: df = add_chexnet(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def threshold_predictions(pred, t=0.5):\n",
    "    return pred >= t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def chexnet_df():\n",
    "    values = [\n",
    "        0.8094, \n",
    "        0.9248, \n",
    "        0.8638, \n",
    "        0.7345, \n",
    "        0.8676, \n",
    "        0.7802, \n",
    "        0.7680, \n",
    "        0.8887, \n",
    "        0.7901, \n",
    "        0.8878, \n",
    "        0.9371, \n",
    "        0.8047, \n",
    "        0.8062, \n",
    "        0.9164\n",
    "    ]\n",
    "    indices = [\n",
    "        \"Atelectasis\", \n",
    "        \"Cardiomegaly\", \n",
    "        \"Effusion\", \n",
    "        \"Infiltration\", \n",
    "        \"Mass\", \n",
    "        \"Nodule\", \n",
    "        \"Pneumonia\", \n",
    "        \"Pneumothorax\", \n",
    "        \"Consolidation\", \n",
    "        \"Edema\", \n",
    "        \"Emphysema\", \n",
    "        \"Fibrosis\", \n",
    "        \"Pleural_Thickening\", \n",
    "        \"Hernia\"\n",
    "    ]\n",
    "    return pd.DataFrame(\n",
    "        values,\n",
    "        index=indices,\n",
    "        columns=[\"CheXNet\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_chexnet(df):\n",
    "    if \"CheXNet\" not in df.columns:\n",
    "        return pd.concat([df, chexnet_df()], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_results(df):\n",
    "    df.to_csv(get_working_dir()/\"AUROC_results.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_results():\n",
    "    return pd.read_csv(get_working_dir()/\"AUROC_results.csv\", index_col=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def array_info(arr, with_hist=False):\n",
    "    print(\"Shape:\\t\", arr.shape)\n",
    "    print(\"Mean:\\t\", arr.mean())\n",
    "    print(\"Std:\\t\", arr.std())\n",
    "    print(\"Max:\\t\", arr.max())\n",
    "    print(\"Min:\\t\", arr.min())\n",
    "    if with_hist == True:\n",
    "        print(\"Histogram:\")\n",
    "        plt.hist(arr.flatten());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_image(fn):\n",
    "    im = plt_image.imread(fn)\n",
    "    plt.imshow(im, cmap=\"bone\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calc_stat(fn):\n",
    "        image = plt_image.imread(fn)\n",
    "        return np.array([image.mean(), image.std()])\n",
    "def calc_stats(df):\n",
    "    image_names = [get_image_path()/fn for fn in df.loc[:,\"Image Index\"]]\n",
    "    with mp.Pool() as p:\n",
    "        stats = p.map(calc_stat, image_names)\n",
    "    stats = np.array(stats)\n",
    "    return stats[:,0].mean(), stats[:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EmptyScheduler: \n",
    "    def step(self):pass\n",
    "    def reset(self):pass\n",
    "    def is_empty(self):return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# From: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n",
    "def find_lr(net, dl, optimizer, criterion, init_value = 1e-8, \n",
    "            final_value=10., beta = 0.98, device=torch.device('cuda:0')):\n",
    "    torch.cuda.empty_cache()\n",
    "    num = len(dl)-1\n",
    "    mult = (final_value / init_value) ** (1/num)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    \n",
    "    net.to(device)\n",
    "    \n",
    "    for data in tqdm(dl):\n",
    "        batch_num += 1\n",
    "        #As before, get the loss for this mini-batch of inputs/outputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #Compute the smoothed loss\n",
    "        avg_loss = beta * avg_loss + (1-beta) *loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "        #Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 10 * best_loss:\n",
    "            return log_lrs, losses\n",
    "        #Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num==1:\n",
    "            best_loss = smoothed_loss\n",
    "        #Store the values\n",
    "        losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        #Do the SGD step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #Update the lr for the next step\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "    return log_lrs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Modified from: https://github.com/dkumazaw/onecyclelr/blob/master/onecyclelr.py\n",
    "class OneCycleLR:\n",
    "    \"\"\" Sets the learing rate of each parameter group by the one cycle learning rate policy\n",
    "    proposed in https://arxiv.org/pdf/1708.07120.pdf. \n",
    "    It is recommended that you set the max_lr to be the learning rate that achieves \n",
    "    the lowest loss in the learning rate range test, and set min_lr to be 1/10 th of max_lr.\n",
    "    So, the learning rate changes like min_lr -> max_lr -> min_lr -> final_lr, \n",
    "    where final_lr = min_lr * reduce_factor.\n",
    "    Note: Currently only supports one parameter group.\n",
    "    Args:\n",
    "        optimizer:             (Optimizer) against which we apply this scheduler\n",
    "        num_steps:             (int) of total number of steps/iterations\n",
    "        lr_range:              (tuple) of min and max values of learning rate\n",
    "        momentum_range:        (tuple) of min and max values of momentum\n",
    "        annihilation_frac:     (float), fracion of steps to annihilate the learning rate\n",
    "        reduce_factor:         (float), denotes the factor by which we annihilate the learning rate at the end\n",
    "        last_step:             (int), denotes the last step. Set to -1 to start training from the beginning\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = OneCycleLR(optimizer, num_steps=num_steps, lr_range=(0.1, 1.))\n",
    "        >>> for epoch in range(epochs):\n",
    "        >>>     for step in train_dataloader:\n",
    "        >>>         train(...)\n",
    "        >>>         scheduler.step()\n",
    "    Useful resources:\n",
    "        https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6\n",
    "        https://medium.com/vitalify-asia/whats-up-with-deep-learning-optimizers-since-adam-5c1d862b9db0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer: Optimizer,\n",
    "                 num_steps: int,\n",
    "                 lr_range: tuple = (0.1, 1.),\n",
    "                 momentum_range: tuple = (0.85, 0.95),\n",
    "                 annihilation_frac: float = 0.1,\n",
    "                 reduce_factor: float = 0.01,\n",
    "                 last_step: int = -1):\n",
    "        # Sanity check\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.min_lr, self.max_lr = lr_range[0], lr_range[1]\n",
    "        assert self.min_lr < self.max_lr, \\\n",
    "            \"Argument lr_range must be (min_lr, max_lr), where min_lr < max_lr\"\n",
    "\n",
    "        self.min_momentum, self.max_momentum = momentum_range[0], momentum_range[1]\n",
    "        assert self.min_momentum < self.max_momentum, \\\n",
    "            \"Argument momentum_range must be (min_momentum, max_momentum), where min_momentum < max_momentum\"\n",
    "\n",
    "        self.num_cycle_steps = int(num_steps * (1. - annihilation_frac))  # Total number of steps in the cycle\n",
    "        self.final_lr = self.min_lr * reduce_factor\n",
    "\n",
    "        self.last_step = last_step\n",
    "\n",
    "        if self.last_step == -1:\n",
    "            self.step()\n",
    "        \n",
    "        self.ground_state = self.state_dict()\n",
    "    \n",
    "    def reset(self):\n",
    "        for k, v in self.ground_state.items():\n",
    "            self.__dict__[k] = v\n",
    "    \n",
    "    def is_empty(self): return False\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer. (Borrowed from _LRScheduler class in torch.optim.lr_scheduler.py)\n",
    "        \"\"\"\n",
    "        return {key: value for key, value in self.__dict__.items() if key not in ['optimizer', 'ground_state']}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the schedulers state. (Borrowed from _LRScheduler class in torch.optim.lr_scheduler.py)\n",
    "        Arguments:\n",
    "            state_dict (dict): scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "        self.__dict__.update(state_dict)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def get_momentum(self):\n",
    "        return self.optimizer.param_groups[0]['momentum']\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Conducts one step of learning rate and momentum update\n",
    "        \"\"\"\n",
    "        current_step = self.last_step + 1\n",
    "        self.last_step = current_step\n",
    "\n",
    "        if current_step <= self.num_cycle_steps // 2:\n",
    "            # Scale up phase\n",
    "            scale = current_step / (self.num_cycle_steps // 2)\n",
    "            lr = self.min_lr + (self.max_lr - self.min_lr) * scale\n",
    "            momentum = self.max_momentum - (self.max_momentum - self.min_momentum) * scale\n",
    "        elif current_step <= self.num_cycle_steps:\n",
    "            # Scale down phase\n",
    "            scale = (current_step - self.num_cycle_steps // 2) / (self.num_cycle_steps - self.num_cycle_steps // 2)\n",
    "            lr = self.max_lr - (self.max_lr - self.min_lr) * scale\n",
    "            momentum = self.min_momentum + (self.max_momentum - self.min_momentum) * scale\n",
    "        elif current_step <= self.num_steps:\n",
    "            # Annihilation phase: only change lr\n",
    "            scale = (current_step - self.num_cycle_steps) / (self.num_steps - self.num_cycle_steps)\n",
    "            lr = self.min_lr - (self.min_lr - self.final_lr) * scale\n",
    "            momentum = None\n",
    "        else:\n",
    "            # Exceeded given num_steps: do nothing\n",
    "            return\n",
    "\n",
    "        self.optimizer.param_groups[0]['lr'] = lr\n",
    "        if momentum:\n",
    "            self.optimizer.param_groups[0]['momentum'] = momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CRX8_Data(Dataset):\n",
    "    \n",
    "    def __init__(self, df, image_path, labels, image_size=None, transforms=None):\n",
    "        self.df = df\n",
    "        self.image_size = image_size\n",
    "        self.image_path = image_path\n",
    "        self.len = df.shape[0]\n",
    "        self.labels = self._correct_labels(labels)\n",
    "        self.transforms = transforms\n",
    "        self.df[\"Index_2\"] = list(range(self.df.shape[0]))\n",
    "        \n",
    "    def _correct_labels(self, lbls):\n",
    "        if type(lbls) == type([]): return lbls\n",
    "        return [lbls]\n",
    "        \n",
    "    def __len__(self): return self.len\n",
    "    \n",
    "    def _resize(self, im):\n",
    "        return transforms.Resize(self.image_size)(im)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self._get_image_path(idx)     \n",
    "        image = read_image(img_path)\n",
    "        image = self._make3D(image)\n",
    "        label = self.df.iloc[idx,:].loc[self.labels].values\n",
    "        if self.transforms: image = self.transforms(image)\n",
    "        return image.float(), torch.Tensor(label.astype(np.float)).float()\n",
    "    \n",
    "    def _correct_dims(self, t):\n",
    "        t = t.squeeze()\n",
    "        if len(t.shape) == 2: return t\n",
    "        if len(t.shape) == 3: return t[0, :, :]\n",
    "        assert False, \"Check dimensions of loaded images!\"\n",
    "    \n",
    "    def _make3D(self, t):\n",
    "        t = self._correct_dims(t)\n",
    "        t = t.detach().numpy()\n",
    "        t = np.expand_dims(t, axis=2)\n",
    "        t = np.concatenate((t,t,t), axis=2)\n",
    "        reshaped = torch.Tensor(np.einsum(\"whc -> cwh\", t))\n",
    "        reshaped /= 255.\n",
    "        return reshaped\n",
    "    \n",
    "    def _get_image_path(self, idx):\n",
    "        return str(self.image_path/self.df.iloc[idx].loc[\"Image Index\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_device(verbose=True):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        if verbose: print(\"Using the GPU!\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        if verbose: print(\"Using the CPU!\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cpu(verbose=True):\n",
    "    if verbose: print(\"Using the CPU!\")\n",
    "    return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.state = {}\n",
    "    \n",
    "    def get_name(self, phase, name): return f\"{phase}_{name}\"\n",
    "    \n",
    "    def add_metric(self, name, phases=[\"train\", \"val\"]):\n",
    "        for phase in phases: self.state[self.get_name(phase, name)] = []\n",
    "\n",
    "    def add_value(self, name, value, phase):\n",
    "        self.state[self.get_name(phase, name)].append(value)\n",
    "    \n",
    "    def _plot_values(self, name):\n",
    "        train_name = self.get_name(\"train\", name)\n",
    "        val_name = self.get_name(\"val\", name)\n",
    "        plt.plot(\n",
    "            list(range(len(self.state[train_name]))), \n",
    "            self.state[train_name]);\n",
    "        plt.plot(\n",
    "            list(range(len(self.state[val_name]))), \n",
    "            self.state[val_name]);\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        self._plot_values(\"loss\")\n",
    "    \n",
    "    def plot_acc(self):\n",
    "        self._plot_values(\"acc\")\n",
    "    \n",
    "    def plot_auroc(self, phase=\"val\"):\n",
    "        name = self.get_name(phase, \"auroc\")\n",
    "        df = self.state[name][0][0]\n",
    "        for data, _ in self.state[name]:\n",
    "            df = pd.concat([df, data], axis=1)\n",
    "        return add_chexnet(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def auroc_score(y_hat, y, model_name):\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_hat)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    label_threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "    return auc_value, label_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def multi_auroc_score(y_hat, y, model_name, labels=get_labels()):\n",
    "    if type(labels) != type([]): labels = [labels] \n",
    "    score = {l:[] for l in labels}\n",
    "    label_thresholds = {}\n",
    "    for l_idx, l in enumerate(labels):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:, l_idx], y_hat[:, l_idx])\n",
    "        auc_value = auc(fpr, tpr)\n",
    "        score[l].append(auc_value)\n",
    "        label_thresholds[l] = thresholds[np.argmax(tpr - fpr)]\n",
    "    df = pd.DataFrame(score.values(), index=score.keys(), columns=[model_name])\n",
    "    return df, label_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, logger, model_name,\n",
    "                labels=get_labels(), alpha=0.1, num_epochs=25, device=torch.device(\"cuda:0\")):\n",
    "    sigmoid = NN.Sigmoid()\n",
    "    since = time.time()\n",
    "    model.to(device)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    running_acc = 0\n",
    "    \n",
    "    \n",
    "    epoch_looper = tqdm(range(num_epochs))\n",
    "\n",
    "    for epoch in epoch_looper:\n",
    "        for phase in ['train', 'val']:\n",
    "            epoch_looper.set_description(f\"Epoch {epoch} - {phase}\")\n",
    "            if phase == 'train': model.train()  \n",
    "            else: model.eval()   \n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            y_hat, truth = [], []\n",
    "\n",
    "            # Iterate over data.\n",
    "            counter = 0\n",
    "            datalooper = tqdm(dataloaders[phase])\n",
    "            for X, y in datalooper:\n",
    "                datalooper.set_description(f\"{phase}_loss: {running_loss:.03f}\")\n",
    "                \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(X)\n",
    "                    sigmoid_outputs = sigmoid(outputs)\n",
    "                    thresholded = sigmoid_outputs >= 0.5\n",
    "                    loss = criterion(outputs, y)\n",
    "                \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    y_hat = [*y_hat, *sigmoid_outputs.cpu().detach().numpy()]\n",
    "                    truth = [*truth, *y.cpu().detach().numpy()]\n",
    "                \n",
    "                if counter == 0: \n",
    "                    running_loss = loss.item()\n",
    "                else:\n",
    "                    running_loss = loss.item() * alpha + (1 - alpha) * running_loss\n",
    "                \n",
    "                #running_loss += loss.item() * y.size(0)\n",
    "                running_corrects += (thresholded == y).sum() / y.numel()\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "            if phase == 'train': scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_auroc = auroc_score(np.array(y_hat), np.array(truth), f\"{model_name}_e{epoch}\", labels=labels)\n",
    "            \n",
    "            logger.add_value(\"loss\",  epoch_loss,  phase)\n",
    "            logger.add_value(\"acc\",   epoch_acc,   phase)\n",
    "            logger.add_value(\"auroc\", epoch_auroc, phase)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        scheduler.reset()\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_layers(model):\n",
    "    pre_layers = []\n",
    "    for i, data in enumerate(model.named_parameters()):\n",
    "        name, param = data\n",
    "        pre_layers.append((i, name.replace(\".weight\", \"\").replace(\".bias\", \"\")))\n",
    "    pre_layers = list(set(pre_layers))\n",
    "    pre_layers = sorted(pre_layers, key=lambda p: p[0])\n",
    "\n",
    "    layers = []\n",
    "    already_added = set()\n",
    "    for i, layer_name in pre_layers:\n",
    "        if layer_name not in already_added:\n",
    "            already_added.add(layer_name)\n",
    "            layers.append((len(layers), layer_name))\n",
    "            \n",
    "    return layers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_batch(idx, dl):\n",
    "    assert idx < len(dl)\n",
    "    for i, data in enumerate(dl):\n",
    "        if i == idx: return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_history(history, mode=\"val\", metric=\"loss\"):\n",
    "    train_losses, val_losses = [], []\n",
    "    for k, v in history.items():\n",
    "        if \"train\" in k: train_losses = [*train_losses, *v[metric]]\n",
    "        if \"val\" in k: val_losses = [*val_losses, *v[metric]]\n",
    "    if mode == \"val\": plt.plot(list(range(len(val_losses))), val_losses)\n",
    "    else: plt.plot(list(range(len(train_losses))), train_losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train_one_cycle(model, \n",
    "                    criterion, \n",
    "                    optimizer, \n",
    "                    scheduler, \n",
    "                    dataloader, \n",
    "                    model_name, \n",
    "                    labels=get_labels(), \n",
    "                    device=torch.device(\"cuda:0\")):\n",
    "    \n",
    "    sigmoid = NN.Sigmoid()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    running_loss, running_acc = [], []\n",
    "    running_predictions_np, running_y_np = [], []\n",
    "    \n",
    "    datalooper = tqdm(dataloader)\n",
    "    for X, y in datalooper:\n",
    "        if len(running_loss) > 0: info = f\"Loss: {running_loss[-1]:.03f}, Acc: {running_acc[-1]:.03f}, lr: {optimizer.param_groups[0]['lr']:.08f}\"\n",
    "        else: info = \"no data yet\"\n",
    "        datalooper.set_description(info)\n",
    "        \n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(X)\n",
    "        predictions = sigmoid(logits)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        predictions_np = predictions.cpu().detach().numpy()\n",
    "        y_np = y.cpu().detach().numpy()\n",
    "        running_predictions_np = [*running_predictions_np, *predictions_np]\n",
    "        running_y_np = [*running_y_np, *y_np]\n",
    "        \n",
    "        # Loss\n",
    "        running_loss.append(loss.item())\n",
    "        \n",
    "        # Accuracy\n",
    "        thresholded = predictions_np >= 0.5\n",
    "        running_acc.append(accuracy_score(y_np, thresholded))\n",
    "        \n",
    "        # AUROC\n",
    "        #mean_auroc = auroc_score(predictions_np, y_np, model_name, labels)[0][model_name].mean()\n",
    "        #running_auroc.append(0.)#mean_auroc)\n",
    "    \n",
    "    running_loss = np.array(running_loss)\n",
    "    running_acc = np.array(running_acc)\n",
    "    #running_auroc = np.array(running_auroc)\n",
    "    epoch_auroc, _ = auroc_score(np.array(running_predictions_np), np.array(running_y_np), model_name)\n",
    "    epoch_info = f\"Loss: {running_loss.mean():.03f}, Acc: {running_acc.mean():.03f}, AUROC: {epoch_auroc:.03f}\"\n",
    "    print(\"Train:\", epoch_info)\n",
    "    \n",
    "    return model, {\"loss\": running_loss, \"acc\": running_acc, \"auroc\": epoch_auroc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def validate(model, \n",
    "             criterion, \n",
    "             dataloader, \n",
    "             model_name,\n",
    "             device=torch.device(\"cuda:0\")):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    sigmoid = NN.Sigmoid()\n",
    "    \n",
    "    running_loss, running_y_hat, running_y = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        datalooper = tqdm(dataloader)\n",
    "        for X, y in datalooper:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            predictions = sigmoid(logits)\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            running_loss.append(loss.item())\n",
    "            running_y_hat = [*running_y_hat, *predictions.cpu().detach().numpy()]\n",
    "            running_y = [*running_y, *y.cpu().detach().numpy()]\n",
    "    \n",
    "    running_loss = np.array(running_loss)\n",
    "    running_y_hat = np.array(running_y_hat)\n",
    "    running_y = np.array(running_y)\n",
    "    \n",
    "    auroc, threshold = auroc_score(running_y_hat, running_y, model_name)\n",
    "    acc = accuracy_score(running_y, (running_y_hat>threshold))\n",
    "    \n",
    "    epoch_info = f\"Loss: {running_loss.mean():.03f}, Acc: {acc:.03f}, AUROC: {auroc:.03f}\"\n",
    "    print(\"Val:\", epoch_info)\n",
    "    \n",
    "    return {\"loss\": running_loss, \"acc\": acc, \"auroc\": auroc,\"threshold\": threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fit(model, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        dataloaders, \n",
    "        model_name, \n",
    "        epochs,\n",
    "        lr,\n",
    "        sam=False,\n",
    "        with_reset=False,\n",
    "        patience=1,\n",
    "        scheduler=EmptyScheduler(), \n",
    "        labels=get_labels(), \n",
    "        metric=\"loss\",\n",
    "        device=torch.device(\"cuda:0\")):\n",
    "    \n",
    "    metric = metric.lower()\n",
    "    assert metric in [\"auroc\", \"loss\", \"acc\"]\n",
    "    if metric in [\"auroc\", \"acc\"]: best_metric = -1e12\n",
    "    if metric == \"loss\": best_metric = 1e12\n",
    "        \n",
    "    patience_counter = 0\n",
    "    patience_metric = 1e12 # monitors val_loss\n",
    "    curr_lr = lr\n",
    "        \n",
    "    history = {}\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        print(f\"Epoch {e+1}:\")\n",
    "        if sam:\n",
    "            model, train_hist = train_SAM(model, criterion, optimizer,\n",
    "                                          dataloaders[\"train\"], model_name, \n",
    "                                          lr=curr_lr, device=device)\n",
    "        else:\n",
    "            model, train_hist = train_one_cycle(model, criterion, optimizer, scheduler, \n",
    "                                       dataloaders[\"train\"], model_name, device=device)\n",
    "        val_hist = validate(model, criterion, dataloaders[\"val\"], model_name, device=device)\n",
    "        \n",
    "        # Reducing lr on plateau\n",
    "        if val_hist[\"loss\"].mean() < patience_metric:\n",
    "            patience_metric = val_hist[\"loss\"].mean()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            patience_counter = 0\n",
    "            curr_lr /= 10.\n",
    "            if not scheduler.is_empty():\n",
    "                scheduler = get_one_cycle_scheduler(dataloaders[\"train\"], curr_lr, optimizer)\n",
    "            elif sam:\n",
    "                sam_optimizer = SAM(model.parameters(), torch.optim.Adam, lr=curr_lr)\n",
    "            else:\n",
    "                optimizer.param_groups[0]['lr'] = curr_lr\n",
    "            if with_reset:\n",
    "                model = load_model(model_name)\n",
    "                print(\"Resetted model to previous best.\")\n",
    "            print(f\"Lowered lr to {curr_lr}\")\n",
    "            \n",
    "        \n",
    "        # Saving model\n",
    "        if metric == \"acc\":\n",
    "            if best_metric < val_hist[\"acc\"]:\n",
    "                best_metric = val_hist[\"acc\"]\n",
    "                save_model(model, model_name)\n",
    "                print(f\"Saved model with acc {best_metric:.04f}\")\n",
    "        elif metric == \"auroc\":\n",
    "            if best_metric < val_hist[\"auroc\"]:\n",
    "                best_metric = val_hist[\"auroc\"]\n",
    "                save_model(model, model_name)\n",
    "                print(f\"Saved model with auroc {best_metric:.04f}\")\n",
    "        elif metric == \"loss\":\n",
    "            if best_metric > val_hist[\"loss\"].mean():\n",
    "                best_metric = val_hist[\"loss\"].mean()\n",
    "                save_model(model, model_name)\n",
    "                print(f\"Saved model with loss {best_metric:.04f}\")\n",
    "        \n",
    "        history[f\"e{e+1}_train\"] = train_hist\n",
    "        history[f\"e{e+1}_val\"] = val_hist\n",
    "        scheduler.reset()\n",
    "        \n",
    "        # Early Stopping\n",
    "        if curr_lr < 1e-13: \n",
    "            print(\"Learning rate is basically zero. Stopping training.\")\n",
    "            break\n",
    "    \n",
    "    model = load_model(model_name)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_one_cycle_scheduler(dataloader, lr, optimizer):\n",
    "    num_steps = len(dataloader)\n",
    "    lr_range = (lr / 10, lr)\n",
    "    return OneCycleLR(optimizer, num_steps, lr_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_binary_df(column, df):\n",
    "    lbls = get_labels()\n",
    "    assert column in lbls\n",
    "    excluding = [lbl for lbl in lbls if lbl != column]\n",
    "    df = df.drop(columns=excluding)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_preclinic_df(df):\n",
    "    if NO_FINDING in df.columns:\n",
    "        df[PATHOLOGIC] = df[NO_FINDING].values\n",
    "        df = df.drop(columns=[NO_FINDING, *get_labels()])\n",
    "        return df\n",
    "    tmp_df = df.drop(columns=[\"Image Index\", \"Follow-up #\", \"Height]\", \"y]\", \"Patient ID\", \"Patient Age\", \"Patient Gender\", \"OriginalImage[Width\",  \"OriginalImagePixelSpacing[x\", \"View Position\"])\n",
    "    patho_idx = np.clip(tmp_df.values.sum(axis=1), 0, 1)\n",
    "    df[PATHOLOGIC] = patho_idx\n",
    "    df = df.drop(columns=get_labels())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# From: https://pub.towardsai.net/we-dont-need-to-worry-about-overfitting-anymore-9fb31a154c81\n",
    "import torch\n",
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train_SAM(model, \n",
    "              criterion, \n",
    "              optimizer, \n",
    "              dataloader, \n",
    "              model_name,\n",
    "              lr=1e-3,\n",
    "              scheduler=EmptyScheduler(),\n",
    "              labels=get_labels(), \n",
    "              device=torch.device(\"cuda:0\")):\n",
    "    \n",
    "    sigmoid = NN.Sigmoid()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    running_loss, running_acc = [], []\n",
    "    running_predictions_np, running_y_np = [], []\n",
    "    \n",
    "    datalooper = tqdm(dataloader)\n",
    "    for X, y in datalooper:\n",
    "        if len(running_loss) > 0: info = f\"Loss: {running_loss[-1]:.03f}, Acc: {running_acc[-1]:.03f}, lr: {lr:.08f}\"\n",
    "        else: info = \"no data yet\"\n",
    "        datalooper.set_description(info)\n",
    "        \n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(X)\n",
    "        predictions = sigmoid(logits)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "        \n",
    "        criterion(model(X), y).backward() \n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        predictions_np = predictions.cpu().detach().numpy()\n",
    "        y_np = y.cpu().detach().numpy()\n",
    "        running_predictions_np = [*running_predictions_np, *predictions_np]\n",
    "        running_y_np = [*running_y_np, *y_np]\n",
    "        \n",
    "        # Loss\n",
    "        running_loss.append(loss.item())\n",
    "        \n",
    "        # Accuracy\n",
    "        thresholded = predictions_np >= 0.5\n",
    "        running_acc.append(accuracy_score(y_np, thresholded))\n",
    "    \n",
    "    running_loss = np.array(running_loss)\n",
    "    running_acc = np.array(running_acc)\n",
    "    epoch_auroc, _ = auroc_score(np.array(running_predictions_np), np.array(running_y_np), model_name)\n",
    "    epoch_info = f\"Loss: {running_loss.mean():.03f}, Acc: {running_acc.mean():.03f}, AUROC: {epoch_auroc:.03f}\"\n",
    "    print(\"Train:\", epoch_info)\n",
    "    \n",
    "    return model, {\"loss\": running_loss, \"acc\": running_acc, \"auroc\": epoch_auroc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "pos_images_name = lambda m: f\"{m}_pos_images.npy\"\n",
    "neg_images_name = lambda m: f\"{m}_neg_images.npy\"\n",
    "\n",
    "pos_nt_name = lambda m: f\"{m}_pos_noisetunnnels.npy\"\n",
    "neg_nt_name = lambda m: f\"{m}_neg_noisetunnnels.npy\"\n",
    "\n",
    "pos_probs_name = lambda m: f\"{m}_pos_probs.npy\"\n",
    "neg_probs_name = lambda m: f\"{m}_neg_probs.npy\"\n",
    "\n",
    "pos_truths_name = lambda m: f\"{m}_pos_truths.npy\"\n",
    "neg_truths_name = lambda m: f\"{m}_neg_truths.npy\"\n",
    "\n",
    "def get_insights_path(model_name):\n",
    "    wd = get_working_dir()\n",
    "    dirs = [f.stem for f in wd.ls() if f.is_dir()]\n",
    "    if \"insights\" not in dirs:\n",
    "        (wd/\"insights\").mkdir(mode=0o777, parents=False, exist_ok=False)\n",
    "    d = wd/\"insights\"\n",
    "    dirs = [f.stem for f in d.ls() if f.is_dir()]\n",
    "    if model_name not in dirs:\n",
    "        (d/model_name).mkdir(mode=0o777, parents=False, exist_ok=False)\n",
    "    return d/model_name\n",
    "\n",
    "def save_insights(pos_images, neg_images, \n",
    "                  pos_noise_tunnels, neg_noise_tunnels,\n",
    "                  pos_probs, neg_probs, \n",
    "                  pos_truths, neg_truths,\n",
    "                  model_name):\n",
    "    ipath = get_insights_path(model_name)\n",
    "    \n",
    "    np.save(ipath/pos_images_name(model_name), pos_images)\n",
    "    np.save(ipath/neg_images_name(model_name), neg_images)\n",
    "    \n",
    "    np.save(ipath/pos_nt_name(model_name), pos_noise_tunnels)\n",
    "    np.save(ipath/neg_nt_name(model_name), neg_noise_tunnels)\n",
    "    \n",
    "    np.save(ipath/pos_probs_name(model_name), pos_probs)\n",
    "    np.save(ipath/neg_probs_name(model_name), neg_probs)\n",
    "    \n",
    "    np.save(ipath/pos_truths_name(model_name), pos_truths)\n",
    "    np.save(ipath/neg_truths_name(model_name), neg_truths)\n",
    "    \n",
    "    print(f\"Insights saved to '{ipath}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "model_metrics_name = lambda m: f\"{m}_metrics.csv\"\n",
    "\n",
    "def load_model_metrics(model_name):\n",
    "    model_stem = \"_\".join(model_name.split(\"_\")[:-1])\n",
    "    parent_dir = get_insights_path(model_name).parent\n",
    "    stem_dirs = [d for d in parent_dir.ls() if d.is_dir()]\n",
    "    for idx, stem_dir in enumerate(stem_dirs):\n",
    "        extracted_label = stem_dir.name.split(\"_\")[-1]\n",
    "        complete_model_name = stem_dir/model_metrics_name(f\"{model_stem}_{extracted_label}\")\n",
    "        if idx == 0:\n",
    "            metrics = load_metrics(complete_model_name)\n",
    "        else:\n",
    "            metrics = pd.concat([metrics, load_metrics(complete_model_name)], axis=1)\n",
    "    return metrics\n",
    "\n",
    "def load_metrics(model_name):\n",
    "    absolute_path = len(str(model_name).split(\"/\")) > 1\n",
    "    if not absolute_path:\n",
    "        ipath = get_insights_path(model_name)\n",
    "        saved_metrics = pd.read_csv(str(ipath/model_metrics_name(model_name)))\n",
    "    else:\n",
    "        saved_metrics = pd.read_csv(str(model_name))\n",
    "    tmp = saved_metrics[\"Unnamed: 0\"].values\n",
    "    saved_metrics = saved_metrics.drop(columns=\"Unnamed: 0\")\n",
    "    saved_metrics.index = tmp\n",
    "    return saved_metrics\n",
    "\n",
    "def save_metrics(model_metrics, model_name):\n",
    "    ipath = get_insights_path(model_name)\n",
    "    model_metrics.to_csv(str(ipath/model_metrics_name(model_name)))\n",
    "    print(f\"Saved metrics to '{model_metrics_name(model_name)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
