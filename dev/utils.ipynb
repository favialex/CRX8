{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as plt_image\n",
    "import multiprocessing as mp\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "NO_FINDING = \"No Finding\"\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "SUBSET_MEAN = 0.50589985\n",
    "SUBSET_STD = 0.23221017\n",
    "SMALL_FRAKTION = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def seed_everything(seed=92):\n",
    "    try: random.seed(seed)\n",
    "    except: pass\n",
    "    try: np.random.seed(seed)\n",
    "    except: pass\n",
    "    try: torch.manual_seed(seed)\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ignore_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_working_dir():\n",
    "    return Path(f\"{os.environ.get('HOME')}/work/crx8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_data_path():\n",
    "    return Path(f\"{os.environ.get('HOME')}/.datasets/CRX8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_image_path(): return get_data_path()/\"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "equivalent-arctic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T13:50:37.358681Z",
     "start_time": "2021-02-20T13:50:37.355503Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_train_val_list(): \n",
    "    with open(get_data_path()/\"train_val_list.txt\") as f:\n",
    "        train_val_list = f.readlines()\n",
    "    return [l.replace(\"\\n\", \"\") for l in train_val_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dataframes(reduced=True, small=False):\n",
    "    image_dir = get_image_path()\n",
    "    train_df, test_df = get_label_dfs(reduced=reduced)\n",
    "    X_train, X_valid, y_train, y_valid = get_train_valid(train_df)\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "    valid_df = pd.concat([X_valid, y_valid], axis=1)\n",
    "    \n",
    "    if small:\n",
    "        train_idx = int(train_df.shape[0] * SMALL_FRAKTION)\n",
    "        val_idx = int(valid_df.shape[0] * SMALL_FRAKTION)\n",
    "        test_idx = int(test_df.shape[0] * SMALL_FRAKTION)\n",
    "        \n",
    "        train_df = train_df.iloc[:train_idx,:]\n",
    "        valid_df = valid_df.iloc[:val_idx,:]\n",
    "        test_df = test_df.iloc[:test_idx,:]\n",
    "    \n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_label_dfs(reduced=True):\n",
    "    data = pd.read_csv(get_data_path()/\"Data_Entry_2017_v2020.csv\")\n",
    "    additional_data = data.drop(columns=[\"Image Index\", \"Finding Labels\"])\n",
    "    values = data.values[:,:2]\n",
    "    labels = get_labels(reduced=reduced)\n",
    "    cols = [\"Image Index\", *labels]\n",
    "    col2idx = {c:i for i, c in enumerate(labels)}\n",
    "    arr = np.zeros((values.shape[0], len(labels)))\n",
    "\n",
    "    for row in range(arr.shape[0]):\n",
    "        image_labels = values[row, 1].split(\"|\")\n",
    "        for col, col_name in enumerate(cols):\n",
    "            for lbl in image_labels:\n",
    "                if reduced:\n",
    "                    if lbl == NO_FINDING: continue\n",
    "                arr[row, col2idx[lbl]] = 1\n",
    "\n",
    "    new_data = pd.DataFrame({\"Image Index\": values[:,0]})\n",
    "    new_data = pd.concat([new_data, pd.DataFrame(arr, columns=labels), additional_data], axis=1)\n",
    "    \n",
    "    train_label_df = new_data[new_data[\"Image Index\"].isin(get_train_val_list())]\n",
    "    test_label_df = new_data[new_data[\"Image Index\"].isin(get_test_list()) ]\n",
    "    \n",
    "    return train_label_df, test_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_labels(reduced=True):\n",
    "    data = pd.read_csv(get_data_path()/\"Data_Entry_2017_v2020.csv\")\n",
    "    tmp = data[\"Finding Labels\"].values\n",
    "    labels = set()\n",
    "    for el in tmp:\n",
    "        for l in el.split(\"|\"): labels.add(l)\n",
    "    if reduced: labels = [l for l in list(labels) if l != \"No Finding\"]\n",
    "    return list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_label_df():\n",
    "    warnings.warn(\"Deprecated! Use 'get_label_dfs'!\")\n",
    "    \n",
    "    data = pd.read_csv(get_data_path()/\"Data_Entry_2017_v2020.csv\")\n",
    "    additional_data = data.drop(columns=[\"Image Index\", \"Finding Labels\"])\n",
    "    values = data.values[:,:2]\n",
    "    labels = get_labels()\n",
    "    cols = [\"Image Index\", *labels]\n",
    "    col2idx = {c:i for i, c in enumerate(labels)}\n",
    "    arr = np.zeros((values.shape[0], len(labels)))\n",
    "\n",
    "    for row in range(arr.shape[0]):\n",
    "        image_labels = values[row, 1].split(\"|\")\n",
    "        for col, col_name in enumerate(cols):\n",
    "            for lbl in image_labels:\n",
    "                arr[row, col2idx[lbl]] = 1\n",
    "\n",
    "    new_data = pd.DataFrame({\"Image Index\": values[:,0]})\n",
    "    new_data = pd.concat([new_data, pd.DataFrame(arr, columns=labels), additional_data], axis=1)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def check_for_leakage(df1, df2):\n",
    "    patient_col = \"Patient ID\"\n",
    "    df1_patients_unique = set(df1[patient_col].values)\n",
    "    df2_patients_unique = set(df2[patient_col].values)\n",
    "    patients_in_both_groups = df1_patients_unique.intersection(df2_patients_unique)\n",
    "    leakage = len(patients_in_both_groups) > 0\n",
    "    return leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_train_valid(data, val_size=0.2, seed=92):\n",
    "    # Currently with patient overlap!\n",
    "    warnings.warn(\"Train-Val-Split currently with patient overlap!\")\n",
    "    labels = get_labels()\n",
    "    X = data[[c for c in data.columns if c not in labels]]\n",
    "    y = data[labels]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=val_size, random_state=seed)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def translate2label(arr):\n",
    "    labels = get_labels()\n",
    "    idx2lbl = {i: l for i, l in enumerate(labels)}\n",
    "    pos_lbls = [idx2lbl[idx] for idx, v in enumerate(arr) if v == 1]\n",
    "    if len(pos_lbls) == 0: return \"No Finding\"\n",
    "    return \"|\".join(pos_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_image(x, y):\n",
    "    x = x * SUBSET_STD + SUBSET_MEAN\n",
    "    plt.title(translate2label(y))\n",
    "    plt.imshow(x, cmap=\"bone\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_batch(X, y):\n",
    "    assert X.shape[0] % 2 == 0\n",
    "    X = X * SUBSET_STD + SUBSET_MEAN\n",
    "    x_dim = int(X.shape[0] / 2)\n",
    "    y_dim = int(X.shape[0] / x_dim)\n",
    "    \n",
    "    axes = []\n",
    "    figure = plt.figure(figsize=(2*x_dim, 20*y_dim))\n",
    "    i = 0\n",
    "    for y_idx in range(y_dim):\n",
    "        for x_idx in range(x_dim):\n",
    "            axes.append(figure.add_subplot(x_dim, y_dim, i+1))\n",
    "            axes[-1].set_title(translate2label(y[i]))\n",
    "            plt.imshow(X[i])\n",
    "            i += 1\n",
    "    figure.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_test_list(): \n",
    "    with open(get_data_path()/\"test_list.txt\") as f:\n",
    "        train_val_list = f.readlines()\n",
    "    return [l.replace(\"\\n\", \"\") for l in train_val_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_train_generator(df, image_dir=get_image_path(), x_col=\"Image Index\", y_cols=get_labels(), shuffle=True, batch_size=8, seed=92, target_w=224, target_h=224):\n",
    "    \"\"\"\n",
    "    Return generator for training set, normalizing using batch\n",
    "    statistics.\n",
    "\n",
    "    Args:\n",
    "      train_df (dataframe): dataframe specifying training data.\n",
    "      image_dir (str): directory where image files are held.\n",
    "      x_col (str): name of column in df that holds filenames.\n",
    "      y_cols (list): list of strings that hold y labels for images.\n",
    "      sample_size (int): size of sample to use for normalization statistics.\n",
    "      batch_size (int): images per batch to be fed into model during training.\n",
    "      seed (int): random seed.\n",
    "      target_w (int): final width of input images.\n",
    "      target_h (int): final height of input images.\n",
    "    \n",
    "    Returns:\n",
    "        train_generator (DataFrameIterator): iterator over training set\n",
    "    \"\"\"        \n",
    "    print(\"getting train generator...\") \n",
    "    # normalize images\n",
    "    image_generator = ImageDataGenerator(\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization= True,\n",
    "        horizontal_flip=True,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        brightness_range=(0.9, 1.1)\n",
    "    )\n",
    "    \n",
    "    # flow from directory with specified batch size\n",
    "    # and target image size\n",
    "    generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_test_and_valid_generator(valid_df, test_df, train_df, image_dir=get_image_path(), x_col=\"Image Index\", y_cols=get_labels(), sample_size=100, batch_size=8, seed=92, target_w=224, target_h=224):\n",
    "    \"\"\"\n",
    "    Return generator for validation set and test test set using \n",
    "    normalization statistics from training set.\n",
    "\n",
    "    Args:\n",
    "      valid_df (dataframe): dataframe specifying validation data.\n",
    "      test_df (dataframe): dataframe specifying test data.\n",
    "      train_df (dataframe): dataframe specifying training data.\n",
    "      image_dir (str): directory where image files are held.\n",
    "      x_col (str): name of column in df that holds filenames.\n",
    "      y_cols (list): list of strings that hold y labels for images.\n",
    "      sample_size (int): size of sample to use for normalization statistics.\n",
    "      batch_size (int): images per batch to be fed into model during training.\n",
    "      seed (int): random seed.\n",
    "      target_w (int): final width of input images.\n",
    "      target_h (int): final height of input images.\n",
    "    \n",
    "    Returns:\n",
    "        test_generator (DataFrameIterator) and valid_generator: iterators over test set and validation set respectively\n",
    "    \"\"\"\n",
    "    print(\"getting train and valid generators...\")\n",
    "    # get generator to sample dataset\n",
    "    raw_train_generator = ImageDataGenerator().flow_from_dataframe(\n",
    "        dataframe=train_df, \n",
    "        directory=image_dir, \n",
    "        x_col=x_col, \n",
    "        y_col=y_cols, \n",
    "        class_mode=\"raw\", \n",
    "        batch_size=sample_size, \n",
    "        shuffle=True, \n",
    "        target_size=(target_w, target_h))\n",
    "    \n",
    "    # get data sample\n",
    "    batch = raw_train_generator.next()\n",
    "    data_sample = batch[0]\n",
    "\n",
    "    # use sample to fit mean and std for test set generator\n",
    "    image_generator = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization= True)\n",
    "    \n",
    "    # fit generator to sample from training data\n",
    "    image_generator.fit(data_sample)\n",
    "\n",
    "    # get test generator\n",
    "    valid_generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=valid_df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "\n",
    "    test_generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=test_df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "    return valid_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_class_freqs(labels):\n",
    "    N = labels.shape[0]\n",
    "    \n",
    "    positive_frequencies = np.sum(labels, axis=0) / N\n",
    "    negative_frequencies = 1 - positive_frequencies\n",
    "\n",
    "    return positive_frequencies, negative_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Return weighted loss function given negative weights and positive weights.\n",
    "\n",
    "    Args:\n",
    "      pos_weights (np.array): array of positive weights for each class, size (num_classes)\n",
    "      neg_weights (np.array): array of negative weights for each class, size (num_classes)\n",
    "    \n",
    "    Returns:\n",
    "      weighted_loss (function): weighted loss function\n",
    "    \"\"\"\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Return weighted loss value. \n",
    "\n",
    "        Args:\n",
    "            y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)\n",
    "            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)\n",
    "        Returns:\n",
    "            loss (Tensor): overall scalar loss summed across all classes\n",
    "        \"\"\"\n",
    "        # initialize loss to zero\n",
    "        loss = 0.0\n",
    "\n",
    "        for i in range(len(pos_weights)):\n",
    "            # for each class, add average weighted loss for that class\n",
    "            loss_pos = -1 * K.mean(pos_weights[i] * y_true[:, i] * K.log(y_pred[:, i] + epsilon))\n",
    "            loss_neg = -1 * K.mean(neg_weights[i] * (1 - y_true[:, i]) * K.log(1 - y_pred[:, i] + epsilon))\n",
    "            loss += loss_pos + loss_neg\n",
    "        \n",
    "        return loss\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def transfer_learn_setup(model, loss, metrics, lr=0.001, n_new_top_layers=1):\n",
    "    for l in model.layers[:-n_new_top_layers]: l.trainable = False\n",
    "    for l in model.layers[-n_new_top_layers:]: l.trainable = True\n",
    "    optim = tf.keras.optimizers.Adam(lr=lr)\n",
    "    model.compile(optimizer=optim, \n",
    "                  loss=loss,\n",
    "                  metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fine_tune_setup(model, loss, metrics, lr=1e-5):\n",
    "    for l in model.layers: l.trainable = True\n",
    "    optim = tf.keras.optimizers.Adam(lr=lr)\n",
    "    model.compile(optimizer=optim, \n",
    "                  loss=loss,\n",
    "                  metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def auroc(y_hat, y, model_name=\"model_name\", with_chexnet=True, with_previous=True):\n",
    "    aurocs = {}\n",
    "    for l_idx, l in enumerate(get_labels()):\n",
    "        try:\n",
    "            v = roc_auc_score(y[:, l_idx], y_hat[:, l_idx])\n",
    "        except ValueError:\n",
    "            warnings.warn(f\"{l} only has one class. Returning 0!\")\n",
    "            v = 0.\n",
    "        aurocs[l] = v\n",
    "    df = pd.DataFrame(aurocs.values(), index=aurocs.keys(), columns=[model_name])\n",
    "    if with_previous:\n",
    "        prev = load_results()\n",
    "        for i, c in enumerate(prev.columns):\n",
    "            if c not in df.columns:\n",
    "                df = pd.concat([df, prev.iloc[:, i]], axis=1)\n",
    "    if with_chexnet: df = add_chexnet(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def threshold_predictions(pred, t=0.5):\n",
    "    return pred >= t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pretty-peoples",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T22:48:19.168705Z",
     "start_time": "2021-03-02T22:48:19.164551Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def chexnet_df():\n",
    "    values = [\n",
    "        0.8094, \n",
    "        0.9248, \n",
    "        0.8638, \n",
    "        0.7345, \n",
    "        0.8676, \n",
    "        0.7802, \n",
    "        0.7680, \n",
    "        0.8887, \n",
    "        0.7901, \n",
    "        0.8878, \n",
    "        0.9371, \n",
    "        0.8047, \n",
    "        0.8062, \n",
    "        0.9164\n",
    "    ]\n",
    "    indices = [\n",
    "        \"Atelectasis\", \n",
    "        \"Cardiomegaly\", \n",
    "        \"Effusion\", \n",
    "        \"Infiltration\", \n",
    "        \"Mass\", \n",
    "        \"Nodule\", \n",
    "        \"Pneumonia\", \n",
    "        \"Pneumothorax\", \n",
    "        \"Consolidation\", \n",
    "        \"Edema\", \n",
    "        \"Emphysema\", \n",
    "        \"Fibrosis\", \n",
    "        \"Pleural_Thickening\", \n",
    "        \"Hernia\"\n",
    "    ]\n",
    "    return pd.DataFrame(\n",
    "        values,\n",
    "        index=indices,\n",
    "        columns=[\"CheXNet\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_chexnet(df):\n",
    "    if \"CheXNet\" not in df.columns:\n",
    "        return pd.concat([df, chexnet_df()], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_results(df):\n",
    "    df.to_csv(get_working_dir()/\"AUROC_results.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_results():\n",
    "    return pd.read_csv(get_working_dir()/\"AUROC_results.csv\", index_col=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def array_info(arr, with_hist=True):\n",
    "    print(\"Shape:\\t\", arr.shape)\n",
    "    print(\"Mean:\\t\", arr.mean())\n",
    "    print(\"Std:\\t\", arr.std())\n",
    "    print(\"Max:\\t\", arr.max())\n",
    "    print(\"Min:\\t\", arr.min())\n",
    "    if with_hist == True:\n",
    "        print(\"Histogram:\")\n",
    "        plt.hist(arr.flatten());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_image(fn):\n",
    "    im = plt_image.imread(fn)\n",
    "    plt.imshow(im, cmap=\"bone\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calc_stat(fn):\n",
    "        image = plt_image.imread(fn)\n",
    "        return np.array([image.mean(), image.std()])\n",
    "def calc_stats(df):\n",
    "    image_names = [get_image_path()/fn for fn in df.loc[:,\"Image Index\"]]\n",
    "    with mp.Pool() as p:\n",
    "        stats = p.map(calc_stat, image_names)\n",
    "    stats = np.array(stats)\n",
    "    return stats[:,0].mean(), stats[:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10: return lr\n",
    "    return lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lr_finder(model, data, lr_0=1e-8, lr_max=10., beta=0.98):\n",
    "    N = len(data) - 1\n",
    "    lr_factor = (lr_max / lr_0) ** (1 / N)\n",
    "    lr = lr_0\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    batch_num = 0\n",
    "    for X, y in data:\n",
    "        batch_num += 1\n",
    "        model.optimizer.lr = lr\n",
    "        loss = model.train_on_batch(X, y=y)\n",
    "\n",
    "        #Smooth loss\n",
    "        avg_loss = beta * avg_loss + (1 - beta) * loss\n",
    "        smooth_loss = avg_loss / (1 - beta ** batch_num)\n",
    "\n",
    "        if batch_num > 1 and smooth_loss > 4 * best_loss:\n",
    "            return log_lrs, losses\n",
    "\n",
    "        if smooth_loss < best_loss or batch_num == 1:\n",
    "            best_loss = smooth_loss\n",
    "\n",
    "        losses.append(smooth_loss)\n",
    "        log_lrs.append(np.log10(lr))\n",
    "\n",
    "        lr *= lr_factor\n",
    "    return log_lrs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_lr_finder(log_lrs, losses):\n",
    "    plt.plot(log_lrs[5:-5], losses[5:-5], scalex=\"log10\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# From: https://www.avanwyk.com/tensorflow-2-super-convergence-with-the-1cycle-policy/\n",
    "class CosineAnnealer:\n",
    "    def __init__(self, start, end, steps):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.steps = steps\n",
    "        self.n = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "        cos = np.cos(np.pi * (self.n / self.steps)) + 1\n",
    "        return self.end + (self.start - self.end) / 2. * cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# From: https://www.avanwyk.com/tensorflow-2-super-convergence-with-the-1cycle-policy/\n",
    "class OneCycleScheduler(Callback):\n",
    "    def __init__(self, lr_max, steps, mom_min=0.85, mom_max=0.95, phase_1_pct=0.3, div_factor=25.):\n",
    "        super(OneCycleScheduler, self).__init__()\n",
    "        lr_min = lr_max / div_factor\n",
    "        final_lr = lr_max / (div_factor * 1e4)\n",
    "        phase_1_steps = steps * phase_1_pct\n",
    "        phase_2_steps = steps - phase_1_steps\n",
    "        \n",
    "        self.phase_1_steps = phase_1_steps\n",
    "        self.phase_2_steps = phase_2_steps\n",
    "        self.phase = 0\n",
    "        self.step = 0\n",
    "        \n",
    "        self.phases = [[CosineAnnealer(lr_min, lr_max, phase_1_steps), CosineAnnealer(mom_max, mom_min, phase_1_steps)], \n",
    "                 [CosineAnnealer(lr_max, final_lr, phase_2_steps), CosineAnnealer(mom_min, mom_max, phase_2_steps)]]\n",
    "        \n",
    "        self.lrs = []\n",
    "        self.moms = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.phase = 0\n",
    "        self.step = 0\n",
    "\n",
    "        self.set_lr(self.lr_schedule().start)\n",
    "        self.set_momentum(self.mom_schedule().start)\n",
    "        \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        self.lrs.append(self.get_lr())\n",
    "        self.moms.append(self.get_momentum())\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.step += 1\n",
    "        if self.step >= self.phase_1_steps:\n",
    "            self.phase = 1\n",
    "            \n",
    "        self.set_lr(self.lr_schedule().step())\n",
    "        self.set_momentum(self.mom_schedule().step())\n",
    "        \n",
    "    def get_lr(self):\n",
    "        try:\n",
    "            return tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "        except AttributeError:\n",
    "            return None\n",
    "        \n",
    "    def get_momentum(self):\n",
    "        try:\n",
    "            return tf.keras.backend.get_value(self.model.optimizer.momentum)\n",
    "        except AttributeError:\n",
    "            return None\n",
    "        \n",
    "    def set_lr(self, lr):\n",
    "        try:\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        except AttributeError:\n",
    "            pass # ignore\n",
    "        \n",
    "    def set_momentum(self, mom):\n",
    "        try:\n",
    "            tf.keras.backend.set_value(self.model.optimizer.momentum, mom)\n",
    "        except AttributeError:\n",
    "            pass # ignore\n",
    "\n",
    "    def lr_schedule(self):\n",
    "        return self.phases[self.phase][0]\n",
    "    \n",
    "    def mom_schedule(self):\n",
    "        return self.phases[self.phase][1]\n",
    "    \n",
    "    def plot(self):\n",
    "        ax = plt.subplot(1, 2, 1)\n",
    "        ax.plot(self.lrs)\n",
    "        ax.set_title('Learning Rate')\n",
    "        ax = plt.subplot(1, 2, 2)\n",
    "        ax.plot(self.moms)\n",
    "        ax.set_title('Momentum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def one_cycle_fits(model, epochs, lr, train_gen, valid_gen, bs, \n",
    "                   model_name, metric_name, mode=\"max\"):\n",
    "    steps = np.ceil(len(train_gen) / bs)\n",
    "    lr_schedule = OneCycleScheduler(lr, steps)\n",
    "    \n",
    "    checkpoint_filepath = str(get_data_path()/\"models\"/model_name)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=False,\n",
    "        monitor=metric_name,\n",
    "        mode=mode,\n",
    "        save_best_only=True)\n",
    "\n",
    "    callbacks = [lr_schedule, model_checkpoint_callback]\n",
    "    \n",
    "    history = {}\n",
    "    for e in range(epochs):\n",
    "        print(f\"Epoch {e+1}/{epochs}\")\n",
    "        training = model.fit(train_gen, \n",
    "                             validation_data=valid_gen,\n",
    "                             epochs=1,\n",
    "                             callbacks=callbacks)\n",
    "        if e == 0:\n",
    "            history = {k:v for k, v in training.history.items()}\n",
    "        else:\n",
    "            for k, v in training.history.items():\n",
    "                history[k].append(v[-1])\n",
    "    return model, history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (with TensorFlow GPU)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
